{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNUjEp5JxTMvn+tJ1D2SL3o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ha-eunYu/Section_Project/blob/main/pix2pix_v_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjP_bZy4eJN6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from math import log10 # For metric function\n",
        "\n",
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset from ImageFolder\n",
        "class Dataset(data.Dataset): # torch Dataset 상속받기\n",
        "    def __init__(self, image_dir, direction):\n",
        "        super(Dataset, self).__init__() # 초기화 상속\n",
        "        self.direction = direction # \n",
        "        self.a_path = os.path.join(image_dir, \"a\") # a: 건물\n",
        "        self.b_path = os.path.join(image_dir, \"b\") # b: Segmentation Mask\n",
        "        self.image_filenames = [x for x in os.listdir(self.a_path)] \n",
        "        self.transform = transforms.Compose([transforms.Resize((256, 256)), # 이미지 크기\n",
        "                                            transforms.ToTensor(), # Numpy -> Tensor\n",
        "                                             transforms.Normalize(mean=(0.5, 0.5, 0.5), \n",
        "                                                std=(0.5, 0.5, 0.5)) # Normalization : -1 ~ 1 range\n",
        "                                            ])\n",
        "        self.len = len(self.image_filenames)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        # a,b 폴더에서 불러오기\n",
        "        a = Image.open(os.path.join(self.a_path, self.image_filenames[index])).convert('RGB') # 건물 사진\n",
        "        b = Image.open(os.path.join(self.b_path, self.image_filenames[index])).convert('RGB') # Segmentation 사진\n",
        "        \n",
        "        # 이미지 전처리\n",
        "        a = self.transform(a)\n",
        "        b = self.transform(b)\n",
        "        \n",
        "        if self.direction == \"a2b\": # 건물 -> Segmentation\n",
        "            return a, b\n",
        "        else:  # Segmentation -> 건물\n",
        "            return b, a\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "        "
      ],
      "metadata": {
        "id": "RKHt5kf_eM9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_dataset = Dataset(\" \", \"b2a\")\n",
        "# test_dataset = Dataset(\" \", \"b2a\")\n",
        "\n",
        "# train_loader = DataLoader(dataset=train_dataset, num_workers=0, batch_size=1, shuffle=True) # Shuffle\n",
        "# test_loader = DataLoader(dataset=test_dataset, num_workers=0, batch_size=1, shuffle=False)"
      ],
      "metadata": {
        "id": "J-osO3ubevAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -1~1 → 0~1\n",
        "def denorm(x):\n",
        "    out = (x + 1) / 2\n",
        "    return out.clamp(0, 1)\n",
        "\n",
        "def show_images(real_a, real_b, fake_b):\n",
        "    plt.figure(figsize=(30,90))\n",
        "    plt.subplot(131)\n",
        "    plt.imshow(real_a.cpu().data.numpy().transpose(1,2,0))\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    \n",
        "    plt.subplot(132)\n",
        "    plt.imshow(real_b.cpu().data.numpy().transpose(1,2,0))\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    \n",
        "    plt.subplot(133)\n",
        "    plt.imshow(fake_b.cpu().data.numpy().transpose(1,2,0))\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    \n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "WSjSASU4e5Y_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Conv -> Batchnorm -> Activate function Layer\n",
        "\n",
        "def conv(c_in, c_out, k_size, stride=2, pad=1, bn=True, activation='relu'):\n",
        "    layers = []\n",
        "    \n",
        "    # Conv layer\n",
        "    layers.append(nn.Conv2d(c_in, c_out, k_size, stride, pad, bias=False))\n",
        "    \n",
        "    # Batch Normalization\n",
        "    if bn:\n",
        "        layers.append(nn.BatchNorm2d(c_out))\n",
        "    \n",
        "    # Activation\n",
        "    if activation == 'lrelu':\n",
        "        layers.append(nn.LeakyReLU(0.2))\n",
        "    elif activation == 'relu':\n",
        "        layers.append(nn.ReLU())\n",
        "    elif activation == 'tanh':\n",
        "        layers.append(nn.Tanh())\n",
        "    elif activation == 'none':\n",
        "        pass\n",
        "    \n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "# Deconv -> BatchNorm -> Activate function Layer\n",
        "\n",
        "def deconv(c_in, c_out, k_size, stride=2, pad=1, bn=True, activation='lrelu'):\n",
        "    layers = []\n",
        "    \n",
        "    # Deconv.\n",
        "    layers.append(nn.ConvTranspose2d(c_in, c_out, k_size, stride, pad, bias=False))\n",
        "    \n",
        "    # Batchnorm\n",
        "    if bn:\n",
        "        layers.append(nn.BatchNorm2d(c_out))\n",
        "    \n",
        "    # Activation\n",
        "    if activation == 'lrelu':\n",
        "        layers.append(nn.LeakyReLU(0.2))\n",
        "    elif activation == 'relu':\n",
        "        layers.append(nn.ReLU())\n",
        "    elif activation == 'tanh':\n",
        "        layers.append(nn.Tanh())\n",
        "    elif activation == 'none':\n",
        "        pass\n",
        "                \n",
        "    return nn.Sequential(*layers)"
      ],
      "metadata": {
        "id": "op1_XPlBfXmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    # initializers\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        # Unet encoder\n",
        "        self.conv1 = conv(3, 64, 4, bn=False, activation='lrelu') # (B, 64, 128, 128)\n",
        "        self.conv2 = conv(64, 128, 4, activation='lrelu') # (B, 128, 64, 64)\n",
        "        self.conv3 = conv(128, 256, 4, activation='lrelu') # (B, 256, 32, 32)\n",
        "        self.conv4 = conv(256, 512, 4, activation='lrelu') # (B, 512, 16, 16)\n",
        "        self.conv5 = conv(512, 512, 4, activation='lrelu') # (B, 512, 8, 8)\n",
        "        self.conv6 = conv(512, 512, 4, activation='lrelu') # (B, 512, 4, 4)\n",
        "        self.conv7 = conv(512, 512, 4, activation='lrelu') # (B, 512, 2, 2)\n",
        "        self.conv8 = conv(512, 512, 4, bn=False, activation='relu') # (B, 512, 1, 1)\n",
        "\n",
        "        # Unet decoder\n",
        "        self.deconv1 = deconv(512, 512, 4, activation='relu') # (B, 512, 2, 2)\n",
        "        self.deconv2 = deconv(1024, 512, 4, activation='relu') # (B, 512, 4, 4)\n",
        "        self.deconv3 = deconv(1024, 512, 4, activation='relu') # (B, 512, 8, 8) \n",
        "        self.deconv4 = deconv(1024, 512, 4, activation='relu') # (B, 512, 16, 16)\n",
        "        self.deconv5 = deconv(1024, 256, 4, activation='relu') # (B, 256, 32, 32)\n",
        "        self.deconv6 = deconv(512, 128, 4, activation='relu') # (B, 128, 64, 64)\n",
        "        self.deconv7 = deconv(256, 64, 4, activation='relu') # (B, 64, 128, 128)\n",
        "        self.deconv8 = deconv(128, 3, 4, activation='tanh') # (B, 3, 256, 256)\n",
        "\n",
        "    # forward method\n",
        "    def forward(self, input):\n",
        "        # Unet encoder\n",
        "        e1 = self.conv1(input)\n",
        "        e2 = self.conv2(e1)\n",
        "        e3 = self.conv3(e2)\n",
        "        e4 = self.conv4(e3)\n",
        "        e5 = self.conv5(e4)\n",
        "        e6 = self.conv6(e5)\n",
        "        e7 = self.conv7(e6)\n",
        "        e8 = self.conv8(e7)\n",
        "                              \n",
        "        # Unet decoder\n",
        "        d1 = F.dropout(self.deconv1(e8), 0.5, training=True)\n",
        "        d2 = F.dropout(self.deconv2(torch.cat([d1, e7], 1)), 0.5, training=True)\n",
        "        d3 = F.dropout(self.deconv3(torch.cat([d2, e6], 1)), 0.5, training=True)\n",
        "        d4 = self.deconv4(torch.cat([d3, e5], 1))\n",
        "        d5 = self.deconv5(torch.cat([d4, e4], 1))\n",
        "        d6 = self.deconv6(torch.cat([d5, e3], 1))\n",
        "        d7 = self.deconv7(torch.cat([d6, e2], 1))\n",
        "        output = self.deconv8(torch.cat([d7, e1], 1))\n",
        "        \n",
        "        return output"
      ],
      "metadata": {
        "id": "HU1HsddLft1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# patch 보고 판별하는 D\n",
        "class Discriminator(nn.Module):\n",
        "    # initializers\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.conv1 = conv(6, 64, 4, bn=False, activation='lrelu')\n",
        "        self.conv2 = conv(64, 128, 4, activation='lrelu')\n",
        "        self.conv3 = conv(128, 256, 4, activation='lrelu')\n",
        "        self.conv4 = conv(256, 512, 4, 1, 1, activation='lrelu')\n",
        "        self.conv5 = conv(512, 1, 4, 1, 1, activation='none')\n",
        "\n",
        "    # forward method\n",
        "    def forward(self, input):\n",
        "        out = self.conv1(input)\n",
        "        out = self.conv2(out)\n",
        "        out = self.conv3(out)\n",
        "        out = self.conv4(out)\n",
        "        out = self.conv5(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "DZ-FSTMugv1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# G, D GPU로\n",
        "G = Generator().cuda()\n",
        "D = Discriminator().cuda()\n",
        "\n",
        "criterionL1 = nn.L1Loss().cuda()\n",
        "criterionMSE = nn.MSELoss().cuda()\n",
        "\n",
        "# Setup optimizer\n",
        "g_optimizer = optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "d_optimizer = optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))"
      ],
      "metadata": {
        "id": "3upWp3DlhSEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "for epoch in range(1, 100):\n",
        "    for i, (real_a, real_b) in enumerate(train_loader, 1):\n",
        "        # forward\n",
        "        real_a, real_b = real_a.cuda(), real_b.cuda()\n",
        "        real_label = torch.ones(1).cuda()\n",
        "        fake_label = torch.zeros(1).cuda()\n",
        "        \n",
        "        fake_b = G(real_a) # G가 생성한 fake\n",
        "        \n",
        "        #============= Train the discriminator =============#\n",
        "        # train with fake\n",
        "        fake_ab = torch.cat((real_a, fake_b), 1)\n",
        "        pred_fake = D.forward(fake_ab.detach())\n",
        "        loss_d_fake = criterionMSE(pred_fake, fake_label)\n",
        "\n",
        "        # train with real\n",
        "        real_ab = torch.cat((real_a, real_b), 1)\n",
        "        pred_real = D.forward(real_ab)\n",
        "        loss_d_real = criterionMSE(pred_real, real_label)\n",
        "        \n",
        "        # Combined D loss\n",
        "        loss_d = (loss_d_fake + loss_d_real) * 0.5\n",
        "        \n",
        "        # Backprop + Optimize\n",
        "        D.zero_grad()\n",
        "        loss_d.backward()\n",
        "        d_optimizer.step()\n",
        "\n",
        "        #=============== Train the generator ===============#\n",
        "        # First, G(A) should fake the discriminator\n",
        "        fake_ab = torch.cat((real_a, fake_b), 1)\n",
        "        pred_fake = D.forward(fake_ab)\n",
        "        loss_g_gan = criterionMSE(pred_fake, real_label)\n",
        "\n",
        "        # Second, G(A) = B\n",
        "        loss_g_l1 = criterionL1(fake_b, real_b) * 10\n",
        "        \n",
        "        loss_g = loss_g_gan + loss_g_l1\n",
        "        \n",
        "        # Backprop + Optimize\n",
        "        G.zero_grad()\n",
        "        D.zero_grad()\n",
        "        loss_g.backward()\n",
        "        g_optimizer.step()\n",
        "        \n",
        "        if i % 200 == 0:\n",
        "            print('======================================================================================================')\n",
        "            print('Epoch [%d/%d], Step[%d/%d], d_loss: %.4f, g_loss: %.4f'\n",
        "                  % (epoch, 100, i, len(train_loader), loss_d.item(), loss_g.item()))\n",
        "            print('======================================================================================================')\n",
        "            show_images(denorm(real_a.squeeze()), denorm(real_b.squeeze()), denorm(fake_b.squeeze()))"
      ],
      "metadata": {
        "id": "enAGNswEhZr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import skimage.color import rgb2lab, lab2rgb\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class Prepoxess_Image(Dataset):\n",
        "  # 생성자\n",
        "  def __init__ (self, paths,mode='train'):\n",
        "    self.mode=mode\n",
        "    self.path=path\n",
        "\n",
        "    # if else문으로 mode가 train인지 아닌지를 설정\n",
        "    # train인 경우, 이미지 사이즈 통일하고 random horizontal flip으로 augmentation 해준다.\n",
        "    # validation인 경우, 이미지 사이즈만 통일하자\n",
        "    \n",
        "    # trasform.Compose 선언!\n",
        "\n",
        "    self.transforms = trainforms.Resize((256,256), Image.BiCUBIC)\n",
        "\n",
        "  def __getitem(self, index):\n",
        "    # 이미지를 불러온다.\n",
        "    pil\n",
        "    # 이미지를 위에서 생성한 transform에 넣어준다.\n",
        "    # 이미지를 넘파이 어레이로 변환한다.\n",
        "    # sklearn rgb2lab lab2rgb\n",
        "    # 이미지를 텐서로 바꿔준다.\n",
        "    # 정류화를 진행한다.\n",
        "  return{'L' : L, 'ab': ab}\n",
        "\n",
        "  def __len(self):\n",
        "    return len(self.paths)"
      ],
      "metadata": {
        "id": "FI51lby4wqJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train = Preprocess_Image(train_paths, mode= 'train')\n",
        "dataloader_train = DataLoader(dataset_train, )"
      ],
      "metadata": {
        "id": "fzxqmK-9yEnd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
